# Magic Image: Reimagining Safety Alignment with An Image

> **Reimagining Safety Alignment with An Image**  
> Yifan Xia\*, Guorui Chen\*, Wenqian Yu\*, Zhijiang Li†, Philip Torr, Jindong Gu†  
> School of Information Management, Wuhan University &nbsp;·&nbsp; Torr Vision Group, University of Oxford  
> \*Equal contribution &nbsp;·&nbsp; †Corresponding authors  
> Contact: lizhijiang@whu.edu.cn · jindong.gu@outlook.com  
> [[Paper]](#) &nbsp;|&nbsp; [[Code]](https://github.com/cysmc/MI-main)

---

## Overview

Large language models (LLMs) and multimodal large language models (MLLMs) face two simultaneous safety challenges:

- **Jailbreak attacks** — adversarial prompts that bypass safety mechanisms and elicit harmful outputs.
- **Over-refusal** — overly conservative alignment that causes models to reject legitimate queries, especially those containing sensitive surface-level features (e.g., *"How do I kill a Python process?"*).

**Magic Image (MI)** is an optimization-driven visual prompt framework that tackles both problems simultaneously. Rather than modifying model weights or decoding strategies, MI optimizes a single image as a parallel input. The optimized image shifts the model's safety decision boundary — reducing unnecessary refusals on borderline queries while strengthening resistance to jailbreak attacks, with negligible impact on clean-data performance.

> **Key Insight:** Visual modality is an underexplored dimension for safety alignment. Even a plain blank image added alongside text already shifts a model's refusal distribution. MI formalizes and maximizes this effect via gradient-based optimization.

---

## How It Works

### Pilot Study

When LLaVA-v1.6-Mistral is given a borderline text prompt (e.g., a sensitive-but-safe question), it frequently refuses. Adding a plain white image to the same prompt significantly reduces the refusal rate — while the refusal rate on actual jailbreak prompts *increases*. This modality interaction is the foundation of Magic Image.

### Dual-Loss Optimization

MI is initialized as a plain image (white, black, gray, or Gaussian noise) and optimized jointly using two cross-entropy objectives:

```
L_dual = λ1 · L_CE( M(x_jail, MI), T_jail )   # encourage refusal on jailbreak inputs
       + λ2 · L_CE( M(x_beni, MI), T_beni )   # encourage response on borderline inputs
```

- **`L_jail`** pushes the model to *refuse* harmful queries when MI is prepended.
- **`L_beni`** pushes the model to *respond helpfully* to over-refused borderline queries.

Gradients flow back through the frozen model into the image pixels. Model weights are never modified.

### Training Data Construction

Target labels are generated by prompting a separate LLM with contextual and few-shot prompts:

| Data type | Source | Target label generation |
|---|---|---|
| Jailbreak (`x_jail`) | Hand-crafted (28 attack types), GCG | Few-shot prompt → refusal response |
| Borderline (`x_beni`) | OR-1k (20% split), XSTest | Contextual prompt → helpful response |

---

## Results

Magic Image achieves the best **SE-score** across all tested models and datasets.

> **SE-score** = Jailbreak Refusal Rate − Borderline Refusal Rate &nbsp;(higher = better balance)

### Main Results (Table 1)

| Model | Method | Borderline ↓ (avg) | Jailbreak ↑ (avg) | SE-score |
|---|---|:---:|:---:|:---:|
| LLaVA-v1.6-Mistral | Default | 14.79 | 36.73 | 20.94 |
| LLaVA-v1.6-Mistral | Safety-Decoding | ~7% | ~51% | 44.24 |
| LLaVA-v1.6-Mistral | **Magic Image** | **4.62** | **65.19** | **60.01** |
| Qwen2-VL | Default | 44.53 | 85.25 | 30.72 |
| Qwen2-VL | **Magic Image** | **23.49** | **88.17** | **66.35** |
| InternVL2.5 | Default | 27.47 | 91.21 | 63.74 |
| InternVL2.5 | **Magic Image** | **2.91** | **91.76** | **89.52** |

### Multimodal Dataset (MOSSBench, Table 3)

| Model | Method | Clean | MOSSBench ↓ |
|---|---|:---:|:---:|
| LLaVA | Default | 2.50 | 14.67 |
| LLaVA | **Magic Image** | **2.00** | **0.33** |
| Qwen | Default | 1.00 | 12.08 |
| Qwen | **Magic Image** | **1.00** | **0** |

### Semantic Quality on Clean Data (Table 7)

| Method | BERT Score | ChatGPT Score |
|---|:---:|:---:|
| Prompt | 61.52 | 83.58 |
| Self-CD | 61.41 | 81.35 |
| SCANS | 50.02 | 73.83 |
| Safety-Decoding | 49.17 | 77.92 |
| **Magic Image** | **64.33** | **87.12** |

---

## Installation

```bash
git clone https://github.com/cysmc/MI-main.git
cd MI-main
pip install -r requirements.txt
```

**Dependencies:**

- Python >= 3.10
- PyTorch >= 2.0 with CUDA
- `transformers`
- `torchvision`
- `tqdm`, `numpy`, `Pillow`, `requests`
- `qwen_vl_utils` (for Qwen models)

---

## Project Structure

```
MI-main/
├── main.py                  # Main training and evaluation entry point
├── utils/
│   ├── llava_processor.py   # LLaVA-specific image preprocessing
│   └── qwen_processor.py    # Qwen-specific image preprocessing
├── train_images/
│   ├── llava/               # Initial seed PNG images for LLaVA optimization
│   └── qwen/                # Initial seed PNG images for Qwen optimization
├── data/
│   ├── train/               # Training data (one xstest JSON + one jailbreak JSON)
│   └── test/                # Evaluation data (one or more JSON files)
├── output/                  # Auto-created output directory
│   ├── test_json/           # Per-epoch inference results (JSON)
│   └── np_loss/             # Per-epoch loss logs (JSON)
└── README.md
```

---

## Data Format

### Training data (`data/train/`)

Two JSON files are expected in `--train_data_dir`:
- A file whose name contains `xstest` → borderline (FRR) samples.
- Any other JSON file → jailbreak (ADV) samples.

Each entry must contain:

```json
{
  "input":  "prompt text sent to the model",
  "output": "target response to optimize toward"
}
```

### Test data (`data/test/`)

Each entry must contain at minimum:

```json
{
  "input": "prompt text"
}
```

An optional `"label"` field is preserved in output results if present.

---

## Usage

### Basic Training

```bash
python main.py \
  --model_names llava-v1.6-mistral \
  --train_data_dir data/train \
  --test_data_dir  data/test \
  --llava_image_root train_images/llava \
  --output_dir output \
  --epoch 3 \
  --batch_size 16 \
  --lr_list 0.2 0.02 0.002 \
  --adv_loss_weight 5.0 \
  --len_data 400 \
  --random_seed 8 \
  --cache_dir model_cache \
  --combination combination1 \
  --train_inf run1
```

### Training Multiple Models

```bash
python main.py \
  --model_names llava-v1.6-mistral llava-v1.6-vicuna Qwen2-vl \
  --llava_image_root train_images/llava \
  --qwen_image_root  train_images/qwen \
  --train_data_dir   data/train \
  --test_data_dir    data/test \
  --output_dir       output \
  --epoch 3 \
  --batch_size 16 \
  --lr_list 0.2 0.02 0.002
```

---

## Arguments

| Argument | Type | Default | Description |
|---|---|---|---|
| `--model_names` | `str+` | `llava-v1.6-mistral` | Model(s) to train. See supported models below. |
| `--cache_dir` | `str` | `model_cache` | HuggingFace model weight cache directory. |
| `--device` | `str` | `cuda:0` | Target CUDA device. |
| `--llava_image_root` | `str` | `train_images/llava` | Directory of seed PNG images for LLaVA optimization. |
| `--qwen_image_root` | `str` | `train_images/qwen` | Directory of seed PNG images for Qwen optimization. |
| `--train_data_dir` | `str` | `data/train` | Training data directory (one xstest JSON + one adv JSON). |
| `--test_data_dir` | `str` | `data/test` | Test data directory (one or more JSON files). |
| `--output_dir` | `str` | `output` | Root output directory; subdirs are created automatically. |
| `--combination` | `str` | `combination1` | Data combination tag used in output directory naming. |
| `--train_inf` | `str` | `run` | Run identifier suffix used in output directory naming. |
| `--len_data` | `int` | `400` | Number of training sample pairs per epoch. |
| `--epoch` | `int` | `3` | Number of training epochs per image per learning rate. |
| `--range` | `int` | `1` | Number of independent runs (dataset re-shuffled each time). |
| `--batch_size` | `int` | `16` | Gradient accumulation batch size. |
| `--lr_list` | `float+` | `0.2 0.02 0.002` | Learning rates to sweep over. |
| `--random_seed` | `int+` | `8` | Random seed(s). |
| `--max_new_tokens` | `int` | `100` | Maximum tokens to generate during evaluation. |
| `--adv_loss_weight` | `float` | `5.0` | Weight `w` for jailbreak loss: `L = L_beni + w * L_jail`. |
| `--access_token` | `str` | `""` | HuggingFace access token for gated models. |

---

## Supported Models

| `--model_names` value | HuggingFace model ID |
|---|---|
| `llava-v1.6-mistral` | `llava-hf/llava-v1.6-mistral-7b-hf` |
| `llava-v1.6-vicuna` | `llava-hf/llava-v1.6-vicuna-7b-hf` |
| `Qwen2-VL-2B` | `Qwen/Qwen2-VL-2B-Instruct` |
| `Qwen2-vl` | `Qwen/Qwen2-VL-7B-Instruct` |
| `Qwen2.5-vl` | `Qwen/Qwen2.5-VL-7B-Instruct` |

---

## Output Structure

After training, the output directory contains:

```
output/
└── {combination}_{model_name}_{train_inf}/
    ├── test_json/
    │   └── run{i}_epoch{e}_{image_name}_{test_file}.json
    └── np_loss/
        └── run{i}_lr{lr}_epoch{e}_{image_name}_loss.json
```

Each inference result file is a JSON list:

```json
[
  {
    "input":     "original prompt",
    "output":    "model response",
    "is_refuse": true,
    "label":     "optional — preserved from test data if present"
  }
]
```

---

## Evaluation Metrics

**Refusal Rate (%)** — proportion of queries rejected by the model, determined via keyword string matching against a predefined refusal word list, including phrases such as *"I cannot"*, *"I don't"*, *"I'm unable"*, *"I apologize"*, *"it is illegal"*, *"that's not allowed"*, and similar expressions.

**SE-score** — Safety-Efficiency balance score:

```
SE-score = R_jail - R_bord
```

Where `R_jail` is the jailbreak refusal rate and `R_bord` is the borderline refusal rate. Higher values indicate a better decoupling of safety from over-refusal.

**BERT Score / ChatGPT Score** — semantic similarity between model outputs on clean data with and without Magic Image, verifying that MI does not degrade normal response quality.

---

## Datasets

| Dataset | Type | Size | Description |
|---|---|:---:|---|
| XSTest | Borderline (eval) | 250 | Benign prompts across 10 categories designed to trigger over-refusal. |
| OKTest | Borderline (eval) | 300 | Safe prompts containing sensitive surface-level terms. |
| OR-1k | Borderline (train/eval) | 1,000 | Hard borderline items across 10 safety domains, previously misjudged by strong models. |
| Hand | Jailbreak (train/eval) | 200 train + 200 test | Hand-crafted attacks spanning 28 attack types, split by category. |
| Hand (trans) | Jailbreak (transfer eval) | 200 | Held-out attack categories unseen during training, for transfer evaluation. |
| GCG | Jailbreak (eval) | — | Filtered GCG adversarial suffixes that successfully bypass the target model. |
| MOSSBench | Multimodal borderline | — | Image-text paired over-refusal benchmark for multimodal models. |
| Pure-Dove | Clean (baseline) | 3,856 | Highly filtered GPT-4 conversations used to monitor clean-data performance. |
| Open-Platypus | Clean (baseline) | — | LLM logical reasoning dataset used to train Platypus2 models. |
| SuperGLUE | Clean (baseline) | — | Difficult language understanding benchmark tasks. |

---

## Key Findings

**Initialization robustness.** Magic Image improves safety alignment regardless of initialization (white, black, gray, Gaussian noise, or natural image). All optimized variants achieve strong final performance. Gray initialization performs slightly best on LLaVA-v1.6-Mistral (borderline: 4.62%, jailbreak: 64.49%).

**Data efficiency.** Meaningful improvements appear with as little as 20% of training data, and performance scales consistently as more data is added.

**Dual-loss is necessary.** Ablation experiments confirm that optimizing with only `L_beni` improves borderline performance but weakens jailbreak defense, and vice versa. The dual-loss strategy achieves global optimality on both dimensions simultaneously.

**Strong generalization.** MI trained on a subset of OR-1k transfers effectively to OKTest and XSTest. Jailbreak defense trained on 10 attack categories transfers to 17 held-out categories.

**Works on multimodal inputs.** For image-text paired data (MOSSBench), MI is reformulated as a universal perturbation generalizing across different input images, reducing over-refusal from ~14% to near-zero.

---

## Limitations

- If a MLLM is inherently insensitive to image modality inputs, Magic Image will have limited effect on its safety behavior.
- When model response habits significantly deviate from the training targets, Magic Image may struggle to shift the model's output distribution effectively.

---

## Citation

```bibtex
@article{xia2024reimagining,
  title   = {Reimagining Safety Alignment with An Image},
  author  = {Xia, Yifan and Chen, Guorui and Yu, Wenqian and Li, Zhijiang
             and Torr, Philip and Gu, Jindong},
  year    = {2024},
  url     = {https://github.com/cysmc/MI-main}
}
```

---

## License

This repository is released for research purposes only. Please use responsibly and in accordance with the terms of the respective model licenses (LLaVA, Qwen).
